{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CgCtb2OwlIv9",
        "BbGYouIxm3Cz",
        "l0D1E2Dlz56h",
        "GZps0f_1yfNK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import uuid\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "\n",
        "from transformers.models.deberta_v2.configuration_deberta_v2 import DebertaV2Config\n",
        "from transformers.models.deberta_v2.modeling_deberta_v2 import (\n",
        "    DebertaV2PreTrainedModel,\n",
        "    DebertaV2Model,\n",
        "    ContextPooler\n",
        ")\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm as auto_tqdm\n"
      ],
      "metadata": {
        "id": "7Z6FqXRylNPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Preprocess\n",
        "In this part, we\n",
        "*   Load train, dev and test data from hugging face and official google drive\n",
        "https://drive.google.com/drive/folders/1Mz8vTnqi7truGrc05v6kWaod6mEK7Enj\n",
        "*   Apply sampler for all three datasets to get smaller subsets, due to the limited computing resources\n",
        "*   Save the sampled data to jsonl files for later usage\n",
        "*   Map the testset's column names for later evaluation, as they originally differ from the train and development sets.\n",
        "\n",
        "Alternatively, you can directly use our preprocessed data by simply download the dataset folder and running the code in the next optional session.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CgCtb2OwlIv9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm6_a7XklHni"
      },
      "outputs": [],
      "source": [
        "# load train & val data from hugging face\n",
        "ds = load_dataset(\"Jinyan1/COLING_2025_MGT_en\")\n",
        "\n",
        "# Extract train and dev sets, train 610767, dev 261758\n",
        "train_set = ds['train']\n",
        "dev_set = ds['dev']\n",
        "\n",
        "print(len(train_set))\n",
        "print(len(dev_set))\n",
        "# Convert to pandas\n",
        "train_df = train_set.to_pandas()\n",
        "dev_df = dev_set.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the test file\n",
        "test_file_path = '/content/test_set_en_with_label.jsonl'\n",
        "\n",
        "# Load the JSONL file into a pandas DataFrame\n",
        "try:\n",
        "    test_df = pd.read_json(test_file_path, lines=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {test_file_path} was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the file: {e}\")\n",
        "\n",
        "print(len(test_df)) #73941"
      ],
      "metadata": {
        "id": "tLBRI7mj_ngL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def equal_stratified_sampler(df, column_name, target_count, random_seed = 42):\n",
        "    unique_values = df[column_name].unique()\n",
        "    num_unique_values = len(unique_values)\n",
        "    samples_per_value = target_count // num_unique_values\n",
        "\n",
        "    sampled_dfs = []\n",
        "    for value in unique_values:\n",
        "        value_df = df[df[column_name] == value]\n",
        "        n_sample = min(samples_per_value, len(value_df))\n",
        "        if n_sample > 0:\n",
        "            sampled = value_df.sample(n=n_sample, random_state=random_seed)\n",
        "            sampled_dfs.append(sampled)\n",
        "            print(f\"  Train - {value}: sampled {n_sample}/{len(value_df)}\")\n",
        "    sampled_df = pd.concat(sampled_dfs).sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
        "    return sampled_df\n",
        "\n",
        "# function for proportional stratified sampling by target column\n",
        "def propotional_stratified_sampler(df, column_name, target_count, random_seed = 42):\n",
        "    unique_values = df[column_name].unique()\n",
        "    total_count = len(df)\n",
        "\n",
        "    sampled_dfs = []\n",
        "    for value in unique_values:\n",
        "        value_df = df[df[column_name] == value]\n",
        "        # Calculate proportional sample size based on class distribution\n",
        "        proportion = len(value_df) / total_count\n",
        "        n_sample = int(target_count * proportion)\n",
        "        n_sample = min(n_sample, len(value_df))\n",
        "\n",
        "        if n_sample > 0:\n",
        "            sampled = value_df.sample(n=n_sample, random_state=random_seed)\n",
        "            sampled_dfs.append(sampled)\n",
        "            print(f\"  {value}: sampled {n_sample}/{len(value_df)} (proportion: {proportion:.2%})\")\n",
        "\n",
        "    sampled_df = pd.concat(sampled_dfs).sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
        "    return sampled_df"
      ],
      "metadata": {
        "id": "nffbcD0M_XKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample data\n",
        "train_data = propotional_stratified_sampler(train_df, 'source', 20000)\n",
        "val_data = propotional_stratified_sampler(dev_df, 'source', 6000)\n",
        "test_data = propotional_stratified_sampler(test_df, 'source', 6000)"
      ],
      "metadata": {
        "id": "UCEtQnVv_X1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data))\n",
        "print(len(val_data))\n",
        "print(len(test_data))"
      ],
      "metadata": {
        "id": "QmDVW4eU8oOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save data for further usage\n",
        "train_data.to_json('/content/sampled_train_data.jsonl', orient='records', lines=True)\n",
        "val_data.to_json('/content/sampled_val_data.jsonl', orient='records', lines=True)\n",
        "test_data.to_json('/content/sampled_test_data.jsonl', orient='records', lines=True)"
      ],
      "metadata": {
        "id": "hk0q6NpKVRaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess sampled testset -- rename colomun, remove empty lines\n",
        "test_path = \"/content/sampled_test_data.jsonl\"\n",
        "output_path = \"/content/sampled_test_data.jsonl\"\n",
        "\n",
        "rows = []\n",
        "with open(test_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            rows.append(json.loads(line))\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(\"[WARN] skip bad line:\", e)\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(\"Original columns:\", df.columns.tolist())\n",
        "\n",
        "# map：language -> lang, domain -> sub_source\n",
        "if \"language\" in df.columns:\n",
        "    df.rename(columns={\"language\": \"lang\"}, inplace=True)\n",
        "if \"domain\" in df.columns:\n",
        "    df.rename(columns={\"domain\": \"sub_source\"}, inplace=True)\n",
        "\n",
        "# generate id\n",
        "if \"id\" not in df.columns:\n",
        "    if \"testset_id\" in df.columns:\n",
        "        df[\"id\"] = df[\"testset_id\"].astype(str)\n",
        "    else:\n",
        "        df[\"id\"] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
        "\n",
        "text_default = \"nodomain\"\n",
        "str_cols = [\"id\", \"source\", \"sub_source\", \"lang\", \"model\", \"text\"]\n",
        "for c in str_cols:\n",
        "    if c not in df.columns:\n",
        "        df[c] = text_default\n",
        "    else:\n",
        "        df[c] = df[c].fillna(text_default).astype(str)\n",
        "\n",
        "#  \"English\"  ->  \"en\"\n",
        "df[\"lang\"] = df[\"lang\"].str.strip().str.lower()\n",
        "df[\"lang\"] = df[\"lang\"].replace({\"english\": \"en\"})\n",
        "\n",
        "# label must be int\n",
        "if \"label\" not in df.columns:\n",
        "    raise ValueError(\"Missing label column in test set.\")\n",
        "df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"label\"])\n",
        "df[\"label\"] = df[\"label\"].astype(int)\n",
        "\n",
        "# keep the same col as val set\n",
        "keep_cols = [\"id\", \"source\", \"sub_source\", \"lang\", \"model\", \"label\", \"text\"]\n",
        "df_out = df[keep_cols]\n",
        "\n",
        "print(\"Unified columns:\", df_out.columns.tolist())\n",
        "print(\"Example row:\\n\", df_out.iloc[0])\n",
        "\n",
        "df_out.to_json(output_path, orient=\"records\", lines=True, force_ascii=False)\n",
        "print(f\"Saved -> {output_path}, rows={len(df_out)}\")"
      ],
      "metadata": {
        "id": "EMTR3uL7Xyj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#(Optional) Load data directly if already exist"
      ],
      "metadata": {
        "id": "BbGYouIxm3Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# file path\n",
        "train_file = '/content/sampled_train_data.jsonl'\n",
        "val_file = '/content/sampled_val_data.jsonl'\n",
        "test_file = '/content/sampled_test_data.jsonl'"
      ],
      "metadata": {
        "id": "UXtNC-Z9m1EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load jsonl file\n",
        "def load_jsonl(file_path):\n",
        "    data_list = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            try:\n",
        "                data = json.loads(line.strip())\n",
        "                # Check if 'label' key exists before appending\n",
        "                if 'label' in data:\n",
        "                    data_list.append(data)\n",
        "                else:\n",
        "                    print(f\"Warning: Skipping item from line {line_num} in {file_path}: 'label' key missing.\")\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Warning: Skipping malformed JSON line {line_num} in {file_path}: {line.strip()} - Error: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Skipping line {line_num} in {file_path} due to unexpected error: {e}\")\n",
        "    return data_list\n",
        "\n",
        "\n",
        "train_data = load_jsonl(train_file)\n",
        "val_data = load_jsonl(val_file)\n",
        "test_data = load_jsonl(test_file)\n",
        "\n",
        "print(f\"=== Data Loaded ===\")\n",
        "print(f\"Train samples: {len(train_data)}\")\n",
        "print(f\"Val samples: {len(val_data)}\")\n",
        "print(f\"Test samples: {len(test_data)}\")\n",
        "print(f\"\\nTrain - Label distribution:\")\n",
        "print(f\"  Human (label=0): {sum(1 for d in train_data if d['label'] == 0)}\")\n",
        "print(f\"  AI (label=1): {sum(1 for d in train_data if d['label'] == 1)}\")\n",
        "print(f\"\\nVal - Label distribution:\")\n",
        "print(f\"  Human (label=0): {sum(1 for d in val_data if d['label'] == 0)}\")\n",
        "print(f\"  AI (label=1): {sum(1 for d in val_data if d['label'] == 1)}\")\n",
        "print(f\"\\nTest - Label distribution:\")\n",
        "print(f\"  Human (label=0): {sum(1 for d in test_data if d['label'] == 0)}\")\n",
        "print(f\"  AI (label=1): {sum(1 for d in test_data if d['label'] == 1)}\")"
      ],
      "metadata": {
        "id": "FbaKSwqInKjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def stats_separate(train_list, val_list, test_list, category = 'sub_source'):\n",
        "    source = category\n",
        "    train_dist = defaultdict(int)\n",
        "    for item in train_list:\n",
        "        sub = item.get(source,'Unknown')\n",
        "        train_dist[sub] += 1\n",
        "\n",
        "    val_dist = defaultdict(int)\n",
        "    for item in val_list:\n",
        "        sub = item.get(source, 'Unknown')\n",
        "        val_dist[sub] += 1\n",
        "\n",
        "    test_dist = defaultdict(int)\n",
        "    for item in test_list:\n",
        "        sub = item.get(source, 'Unknown')\n",
        "        test_dist[sub] += 1\n",
        "\n",
        "\n",
        "    train_total = sum(train_dist.values())\n",
        "    val_total = sum(val_dist.values())\n",
        "    test_total = sum(test_dist.values())\n",
        "\n",
        "    train_val_subs = sorted(set(list(train_dist.keys()) + list(val_dist.keys())))\n",
        "\n",
        "    print(\"=== Train + Val Sub_source Statistics ===\\n\")\n",
        "    print(f\"{'Sub_source':<20} {'Train':<30} {'Val':<30}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for sub in train_val_subs:\n",
        "        train_count = train_dist[sub]\n",
        "        train_pct = (train_count / train_total * 100) if train_total > 0 else 0\n",
        "        train_str = f\"{train_count} ({train_pct:.2f}%)\"\n",
        "\n",
        "        val_count = val_dist[sub]\n",
        "        val_pct = (val_count / val_total * 100) if val_total > 0 else 0\n",
        "        val_str = f\"{val_count} ({val_pct:.2f}%)\"\n",
        "\n",
        "        print(f\"{sub:<20} {train_str:<30} {val_str:<30}\")\n",
        "\n",
        "    print(f\"\\n{'TOTAL':<20} {train_total:<30} {val_total:<30}\\n\")\n",
        "\n",
        "    test_subs = sorted(test_dist.keys())\n",
        "\n",
        "    print(\"=== Test Sub_source Statistics ===\\n\")\n",
        "    print(f\"{'Sub_source':<20} {'Test':<30}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for sub in test_subs:\n",
        "        test_count = test_dist[sub]\n",
        "        test_pct = (test_count / test_total * 100) if test_total > 0 else 0\n",
        "        test_str = f\"{test_count} ({test_pct:.2f}%)\"\n",
        "\n",
        "        print(f\"{sub:<20} {test_str:<30}\")\n",
        "\n",
        "    print(f\"\\n{'TOTAL':<20} {test_total:<30}\\n\")\n",
        "\n",
        "\n",
        "stats_separate(train_data, val_data, test_data, 'sub_source')"
      ],
      "metadata": {
        "id": "esHlNBBUWRxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats_separate(train_data, val_data, test_data, 'model')"
      ],
      "metadata": {
        "id": "x8NkyftObbue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Calculate perplexity\n",
        "\n",
        "In this part, we use the simplified version of calculating DNA-Detect LLM score(more details can be found in [paper](https://arxiv.org/pdf/2509.15550)).\n",
        "\n",
        "For the LLM modelsWe use gpt2-medium and gpt2-large, also EleutherAI/pythia-160m and EleutherAI/pythia-410m.\n",
        "\n",
        "Calulated scores are saved as json files for further usage and evaluation.\n",
        "\n",
        "Part of the code in this section is adapted from [DNA-DetectLLM](https://github.com/Xiaoweizhu57/DNA-DetectLLM).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AnCTMmNWnSJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "2eYzZDQLpKBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate original & repaired perplexity\n",
        "def sum_perplexity(encoding, logits):\n",
        "    # encoding is the original input, use observer's tokenizer\n",
        "    # logits is vocab_size's prob of each position in the text, generated by performer\n",
        "    shifted_logits = logits[..., :-1, :]\n",
        "    attention = encoding.attention_mask[..., 1:]\n",
        "    labels_std = encoding.input_ids[..., 1:]  # token id of original text\n",
        "    labels_max = torch.argmax(shifted_logits, dim=-1)  # token with max prob\n",
        "\n",
        "    logits_T = shifted_logits.transpose(1, 2)\n",
        "\n",
        "    ce_std = F.cross_entropy(logits_T, labels_std, reduction='none')\n",
        "    ce_max = F.cross_entropy(logits_T, labels_max, reduction='none')\n",
        "\n",
        "    attn_sum = attention.sum(dim=1).clamp(min=1)\n",
        "    ppl_std = (ce_std * attention).sum(dim=1) / attn_sum #actual token vs. performer prediction -- how unatural performer think the text originally is\n",
        "    ppl_max = (ce_max * attention).sum(dim=1) / attn_sum\n",
        "\n",
        "    return (ppl_std + ppl_max).cpu().numpy()\n",
        "\n",
        "# calculate cross entropy（observer vs. performer）\n",
        "def entropy(p_logits, q_logits, encoding, pad_token_id):\n",
        "    vocab_size = p_logits.shape[-1]\n",
        "    total_tokens = q_logits.shape[-2]\n",
        "\n",
        "    p_proba = F.softmax(p_logits, dim=-1).view(-1, vocab_size)\n",
        "    q_scores = q_logits.view(-1, vocab_size)\n",
        "\n",
        "    ce = F.cross_entropy(q_scores, p_proba, reduction='none').view(-1, total_tokens)\n",
        "    padding_mask = (encoding.input_ids != pad_token_id).type(torch.uint8)\n",
        "\n",
        "    agg_ce = ((ce * padding_mask).sum(1) / padding_mask.sum(1)).cpu().float().numpy()\n",
        "    return agg_ce"
      ],
      "metadata": {
        "id": "5J0CUOuEpMAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleDNADetectLLM:\n",
        "\n",
        "    def __init__(self, observer_model_name=\"gpt2-medium\", performer_model_name=\"gpt2-xl\"):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        print(f\"load Observer: {observer_model_name}...\")\n",
        "        self.observer_model = AutoModelForCausalLM.from_pretrained(observer_model_name).to(self.device)\n",
        "        self.observer_model.eval()\n",
        "\n",
        "        print(f\"load Performer: {performer_model_name}...\")\n",
        "        self.performer_model = AutoModelForCausalLM.from_pretrained(performer_model_name).to(self.device)\n",
        "        self.performer_model.eval()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(observer_model_name)\n",
        "        if not self.tokenizer.pad_token:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "\n",
        "    def compute_score(self, text):\n",
        "        # Tokenize\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_token_type_ids=False\n",
        "        ).to(self.device)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            observer_logits = self.observer_model(**encoding).logits\n",
        "            performer_logits = self.performer_model(**encoding).logits\n",
        "\n",
        "\n",
        "        ppl = sum_perplexity(encoding, performer_logits)\n",
        "        x_ppl = entropy(observer_logits, performer_logits, encoding, self.tokenizer.pad_token_id)\n",
        "\n",
        "\n",
        "        score = ppl / (2 * x_ppl)\n",
        "\n",
        "        return score[0] if isinstance(score, np.ndarray) else score\n"
      ],
      "metadata": {
        "id": "hrLcolsjp3m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize\n",
        "detector = SimpleDNADetectLLM(\n",
        "    observer_model_name=\"gpt2-medium\",  # 345M\n",
        "    performer_model_name=\"gpt2-large\"      # 774M\n",
        ")"
      ],
      "metadata": {
        "id": "L6nNgFkyPa8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional - another detector\n",
        "detector = SimpleDNADetectLLM(\n",
        "    observer_model_name=\"EleutherAI/pythia-160m\",  # 160M\n",
        "    performer_model_name=\"EleutherAI/pythia-410m\"      # 410M\n",
        ")"
      ],
      "metadata": {
        "id": "DjwKyPAJJoWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate perplexity score\n",
        "def compute_and_save_scores(data_list, output_file, detector):\n",
        "\n",
        "    # if exists, load directly\n",
        "    if os.path.exists(output_file):\n",
        "        print(f\"Loading existing scores from {output_file}\")\n",
        "        with open(output_file, 'r') as f:\n",
        "            scores = json.load(f)\n",
        "        return scores\n",
        "\n",
        "    # otherwise calculate\n",
        "    print(f\"Computing scores for {len(data_list)} samples...\")\n",
        "    scores = []\n",
        "    for item in tqdm(data_list, desc=\"DNA-DETECT\"):\n",
        "      score = detector.compute_score(item['text'])\n",
        "      scores.append(float(score))\n",
        "\n",
        "    # save\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(scores, f)\n",
        "    print(f\"Scores saved to {output_file}\")\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "Bn0vqH1WqC74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_scores = compute_and_save_scores(train_data, '/content/train_dna_scores.json', detector)\n",
        "val_scores = compute_and_save_scores(val_data, '/content/val_dna_scores.json', detector)\n",
        "test_scores = compute_and_save_scores(test_data, '/content/test_dna_scores.json', detector)\n",
        "\n",
        "print(f\"\\n=== Perplexity Statistics ===\")\n",
        "print(f\"Train - Mean: {np.mean(train_scores):.4f}, Std: {np.std(train_scores):.4f}\")\n",
        "print(f\"Val - Mean: {np.mean(val_scores):.4f}, Std: {np.std(val_scores):.4f}\")\n",
        "print(f\"Test - Mean: {np.mean(test_scores):.4f}, Std: {np.std(test_scores):.4f}\")"
      ],
      "metadata": {
        "id": "Zepa8gTqqaVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize for later train\n",
        "# use mean and std of the train set\n",
        "scaler = StandardScaler()\n",
        "train_scores_array = np.array(train_scores).reshape(-1, 1)\n",
        "val_scores_array = np.array(val_scores).reshape(-1, 1)\n",
        "test_scores_array = np.array(test_scores).reshape(-1, 1)\n",
        "\n",
        "\n",
        "# fit and transform\n",
        "train_scores_normalized = scaler.fit_transform(train_scores_array).flatten()\n",
        "val_scores_normalized = scaler.transform(val_scores_array).flatten()\n",
        "test_scores_normalized = scaler.transform(test_scores_array).flatten()\n",
        "\n",
        "\n",
        "print(f\"\\n=== Normalized Perplexity ===\")\n",
        "print(f\"Train - Mean: {train_scores_normalized.mean():.4f}, Std: {train_scores_normalized.std():.4f}\")\n",
        "print(f\"Val - Mean: {val_scores_normalized.mean():.4f}, Std: {val_scores_normalized.std():.4f}\")\n",
        "print(f\"Test - Mean: {test_scores_normalized.mean():.4f}, Std: {test_scores_normalized.std():.4f}\")"
      ],
      "metadata": {
        "id": "xvTpNFGgrnZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) Load score directly if exist, then normalize (repeated code as above)"
      ],
      "metadata": {
        "id": "kDA6DngSvwwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load score directly\n",
        "def read_existed_scores(output_file):\n",
        "  if os.path.exists(output_file):\n",
        "      print(f\"Loading existing scores from {output_file}\")\n",
        "      with open(output_file, 'r') as f:\n",
        "          scores = json.load(f)\n",
        "      return scores"
      ],
      "metadata": {
        "id": "883aVn5Gv2LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_scores = read_existed_scores ('/content/train_dna_scores.json')\n",
        "val_scores = read_existed_scores('/content/val_dna_scores.json')\n",
        "test_scores = read_existed_scores('/content/test_dna_scores.json')"
      ],
      "metadata": {
        "id": "PzcMxjMms9rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize for later train\n",
        "# use mean and std of the train set\n",
        "scaler = StandardScaler()\n",
        "train_scores_array = np.array(train_scores).reshape(-1, 1)\n",
        "val_scores_array = np.array(val_scores).reshape(-1, 1)\n",
        "test_scores_array = np.array(test_scores).reshape(-1, 1)\n",
        "\n",
        "\n",
        "# fit and transform\n",
        "train_scores_normalized = scaler.fit_transform(train_scores_array).flatten()\n",
        "val_scores_normalized = scaler.transform(val_scores_array).flatten()\n",
        "test_scores_normalized = scaler.transform(test_scores_array).flatten()\n",
        "\n",
        "\n",
        "print(f\"\\n=== Normalized Perplexity ===\")\n",
        "print(f\"Train - Mean: {train_scores_normalized.mean():.4f}, Std: {train_scores_normalized.std():.4f}\")\n",
        "print(f\"Val - Mean: {val_scores_normalized.mean():.4f}, Std: {val_scores_normalized.std():.4f}\")\n",
        "print(f\"Test - Mean: {test_scores_normalized.mean():.4f}, Std: {test_scores_normalized.std():.4f}\")"
      ],
      "metadata": {
        "id": "uml_1DcgtB0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model Config\n",
        "In this part we define the new model and dataset, where we add perplexity as an additional input to DebertaV2, and load the weight from a pretrained model (deberta-v3-base-daigenc-mgt1a, more details can be found in [Paper](https://aclanthology.org/2025.genaidetect-1.26.pdf)).\n",
        "\n",
        "Part of the code in this section is adapted from [GitHub](https://github.com/Advacheck-OU/ai-detector-coling2025/tree/main)\n"
      ],
      "metadata": {
        "id": "xgQ0dK-fqp-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset\n",
        "class MGTDatasetWithPerplexity(Dataset):\n",
        "    \"\"\"include perplexity as input\"\"\"\n",
        "    def __init__(self, data_list, perplexity_scores, tokenizer, max_length=512):\n",
        "        self.data_list = data_list\n",
        "        self.perplexity_scores = perplexity_scores\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data_list[idx]\n",
        "\n",
        "        # Tokenize text\n",
        "        encoding = self.tokenizer(\n",
        "            item['text'],\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'perplexity_feature': torch.tensor([self.perplexity_scores[idx]], dtype=torch.float32),\n",
        "            'labels': torch.tensor(item['label'], dtype=torch.long),\n",
        "            'source': item['source'],\n",
        "            'sub_source': item['sub_source'],\n",
        "            'model': item['model']\n",
        "        }"
      ],
      "metadata": {
        "id": "pbNahZkcr8q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pretrained model and transfer the weight\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "\n",
        "pretrained_model_name = \"OU-Advacheck/deberta-v3-base-daigenc-mgt1a\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
        "base_config = AutoConfig.from_pretrained(pretrained_model_name)\n",
        "\n",
        "print(f\"Loaded tokenizer and config from {pretrained_model_name}\")\n",
        "\n",
        "# load the base model for training later\n",
        "from transformers import DebertaV2ForSequenceClassification\n",
        "pretrained_model = DebertaV2ForSequenceClassification.from_pretrained(pretrained_model_name)\n",
        "\n",
        "print(\"Loaded pretrained model for weight transfer\")"
      ],
      "metadata": {
        "id": "VoBacpzMsJSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the new DeBERTa model\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "\n",
        "import transformers\n",
        "from transformers import models\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "\n",
        "from transformers.models.deberta_v2.configuration_deberta_v2 import DebertaV2Config\n",
        "from transformers.models.deberta_v2.modeling_deberta_v2 import (\n",
        "    DebertaV2PreTrainedModel,\n",
        "    DebertaV2Model,\n",
        "    ContextPooler\n",
        ")\n",
        "\n",
        "\n",
        "class DebertaV2WithPerplexity(DebertaV2PreTrainedModel):\n",
        "    \"\"\"DeBERTa + Perplexity\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = 2  # binary classification\n",
        "        self.config = config\n",
        "\n",
        "        # DeBERTa core\n",
        "        self.deberta = DebertaV2Model(config)\n",
        "\n",
        "        drop_out = getattr(config, \"cls_dropout\", None)\n",
        "        drop_out = config.hidden_dropout_prob if drop_out is None else drop_out\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "\n",
        "        self.pooler = ContextPooler(config)\n",
        "\n",
        "        # Classification head：input = hidden_size + 1 (perplexity)\n",
        "        H1, H2 = 512, 256\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size + 1, H1),  # 768+1 → 512\n",
        "            nn.GELU(),\n",
        "            nn.Linear(H1, H2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(H2, self.num_labels)\n",
        "        )\n",
        "\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        perplexity_feature=None,  # newly added param\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # DeBERTa\n",
        "        outputs = self.deberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        encoder_layer = outputs[0]\n",
        "        pooled_output = self.pooler(encoder_layer)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        # map perplexity\n",
        "        if perplexity_feature is not None:\n",
        "            # perplexity_feature: [batch_size, 1]\n",
        "            pooled_output = torch.cat([pooled_output, perplexity_feature], dim=-1)  # [B, 769]\n",
        "        else:\n",
        "            # if not offered, filled with 0\n",
        "            batch_size = pooled_output.size(0)\n",
        "            zero_feature = torch.zeros(batch_size, 1, device=pooled_output.device)\n",
        "            pooled_output = torch.cat([pooled_output, zero_feature], dim=-1)\n",
        "\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "J15HvKW0sVDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create new model\n",
        "new_model = DebertaV2WithPerplexity(base_config)\n",
        "\n",
        "# load weight\n",
        "new_model.deberta.load_state_dict(pretrained_model.deberta.state_dict())\n",
        "new_model.pooler.load_state_dict(pretrained_model.pooler.state_dict())\n",
        "print(\"Transferred DeBERTa and pooler weights\")\n",
        "\n",
        "old_classifier_weight = pretrained_model.classifier.weight.data  # [2, 768]\n",
        "old_classifier_bias = pretrained_model.classifier.bias.data      # [2]\n",
        "\n",
        "# initialize classification head, set weight of perplexity as 0, others use the old weight\n",
        "with torch.no_grad():\n",
        "    # first layer: Linear(769, 512)\n",
        "    first_layer = new_model.classifier[0]  # Linear(769, 512)\n",
        "    nn.init.xavier_uniform_(first_layer.weight[:, :768])\n",
        "    first_layer.weight[:, 768] = 0.0\n",
        "    nn.init.zeros_(first_layer.bias)\n",
        "\n",
        "print(\"Initialized new classifier head (perplexity weights = 0)\")\n",
        "print(f\"Model ready for training. Total parameters: {sum(p.numel() for p in new_model.parameters()):,}\")"
      ],
      "metadata": {
        "id": "1bs0igUKs1ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Baseline\n",
        "We use 2 baselines here\n",
        "*   Perplexity + threshold\n",
        "*   Pretrained model\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xs8i8R3NuQ9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline 1 Perplexity + Threshold"
      ],
      "metadata": {
        "id": "M8QpK4V4xS_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "\n",
        "def evaluate_perplexity_baseline(scores, labels, thresholds=None):\n",
        "    \"\"\"use perplexity score and threshold as classifier\"\"\"\n",
        "    scores = np.array(scores)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    if thresholds is None:\n",
        "        # find the best threshold on train set\n",
        "        best_threshold = None\n",
        "        best_f1 = 0\n",
        "\n",
        "        # try different threshold\n",
        "        for percentile in range(10, 100, 5):\n",
        "            threshold = np.percentile(scores, percentile)\n",
        "            preds = (scores < threshold).astype(int) #human label = 0\n",
        "            f1 = f1_score(labels, preds)\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "\n",
        "        thresholds = [best_threshold]\n",
        "        print(f\"  Best threshold from training: {best_threshold:.4f} (F1={best_f1:.4f})\")\n",
        "\n",
        "    results = []\n",
        "    for threshold in thresholds:\n",
        "        preds = (scores < threshold).astype(int) #human label=0\n",
        "        results.append({\n",
        "            'threshold': threshold,\n",
        "            'accuracy': accuracy_score(labels, preds),\n",
        "            'precision': precision_score(labels, preds, zero_division=0),\n",
        "            'recall': recall_score(labels, preds, zero_division=0),\n",
        "            'f1': f1_score(labels, preds, zero_division=0),\n",
        "        })\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "YXdZQPmfupE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Baseline 1: Perplexity Only ===\")\n",
        "print(\"\\nFinding optimal threshold on train set...\")\n",
        "train_labels = [d['label'] for d in train_data]\n",
        "train_results = evaluate_perplexity_baseline(train_scores, train_labels)\n",
        "optimal_threshold = train_results[0]['threshold']\n",
        "print(f\"train results:{train_results}\")\n",
        "\n",
        "print(f\"\\nEvaluating on validation set with threshold={optimal_threshold:.4f}...\")\n",
        "val_labels = [d['label'] for d in val_data]\n",
        "val_results = evaluate_perplexity_baseline(val_scores, val_labels, thresholds=[optimal_threshold])\n",
        "\n",
        "print(f\"\\nValidation Results:\")\n",
        "print(f\"  Accuracy:  {val_results[0]['accuracy']:.4f}\")\n",
        "print(f\"  Precision: {val_results[0]['precision']:.4f}\")\n",
        "print(f\"  Recall:    {val_results[0]['recall']:.4f}\")\n",
        "print(f\"  F1-Score:  {val_results[0]['f1']:.4f}\")"
      ],
      "metadata": {
        "id": "OYlErKQdwU78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nEvaluating on Test set with threshold={optimal_threshold:.4f}...\")\n",
        "test_labels = [d['label'] for d in test_data]\n",
        "test_results = evaluate_perplexity_baseline(test_scores, test_labels, thresholds=[optimal_threshold])\n",
        "\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"  Accuracy:  {test_results[0]['accuracy']:.4f}\")\n",
        "print(f\"  Precision: {test_results[0]['precision']:.4f}\")\n",
        "print(f\"  Recall:    {test_results[0]['recall']:.4f}\")\n",
        "print(f\"  F1-Score:  {test_results[0]['f1']:.4f}\")"
      ],
      "metadata": {
        "id": "J7h-xAjBwXFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Evaluate by sub-source (on validation set)"
      ],
      "metadata": {
        "id": "g4peajrlwn31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "\n",
        "def evaluate_by_group(data_list, labels, preds, probs=None, group_key=\"sub_source\", model_name=None):\n",
        "    \"\"\"\n",
        "    data_list: list[dict],sub_source/source/model\n",
        "    group_key: 'sub_source'/'source'/'domain'/...\n",
        "    model_name: baseline 1,baselin2\n",
        "    \"\"\"\n",
        "    labels = np.array(labels)\n",
        "    preds = np.array(preds)\n",
        "    probs = None if probs is None else np.array(probs)\n",
        "\n",
        "    assert len(data_list) == len(labels) == len(preds), \"incosisdent length of data_list/labels/preds\"\n",
        "    if probs is not None:\n",
        "        assert len(probs) == len(labels),  \"incosisdent length of probs\"\n",
        "\n",
        "    # group value（fill with 'UNKNOWN' in case of missing）\n",
        "    groups = [d.get(group_key, \"UNKNOWN\") for d in data_list]\n",
        "\n",
        "    rows = []\n",
        "    for g in sorted(set(groups)):\n",
        "        idxs = np.where(np.array(groups) == g)[0]\n",
        "        y = labels[idxs]\n",
        "        p = preds[idxs]\n",
        "\n",
        "        row = {\n",
        "            \"model\": model_name,\n",
        "            group_key: g,\n",
        "            \"count\": int(len(idxs)),\n",
        "            \"accuracy\": float(accuracy_score(y, p)),\n",
        "            \"precision\": float(precision_score(y, p, zero_division=0)),\n",
        "            \"recall\": float(recall_score(y, p, zero_division=0)),\n",
        "            \"f1\": float(f1_score(y, p, zero_division=0)),\n",
        "        }\n",
        "\n",
        "\n",
        "        if probs is not None and len(np.unique(y)) > 1:\n",
        "            row[\"auc\"] = float(roc_auc_score(y, probs[idxs]))\n",
        "        else:\n",
        "            row[\"auc\"] = None\n",
        "\n",
        "        rows.append(row)\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values([\"count\"], ascending=False).reset_index(drop=True)\n",
        "    return df"
      ],
      "metadata": {
        "id": "E4f7zK8hwqfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline1_labels = [d[\"label\"] for d in val_data]\n",
        "baseline1_preds = (np.array(val_scores) < optimal_threshold).astype(int)\n",
        "\n",
        "baseline1_sub_df = evaluate_by_group(\n",
        "    data_list=val_data,\n",
        "    labels=baseline1_labels,\n",
        "    preds=baseline1_preds,\n",
        "    probs=None,\n",
        "    group_key=\"sub_source\",\n",
        "    model_name=\"baseline1\"\n",
        ")\n",
        "display(baseline1_sub_df)"
      ],
      "metadata": {
        "id": "aBMoOaefw6sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save as json\n",
        "with open(\"baseline1_by_sub_source.json\", \"w\") as f:\n",
        "    json.dump(baseline1_sub_df.to_dict(\"records\"), f, indent=2)"
      ],
      "metadata": {
        "id": "uZqpV-DDxBzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate by sub-source (test set)"
      ],
      "metadata": {
        "id": "jEYwHmhU64L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline1_labels = [d[\"label\"] for d in test_data]\n",
        "baseline1_preds = (np.array(test_scores) < optimal_threshold).astype(int)\n",
        "baseline1_sub_test_df = evaluate_by_group(\n",
        "    data_list=test_data,\n",
        "    labels=baseline1_labels,\n",
        "    preds=baseline1_preds,\n",
        "    probs=None,\n",
        "    group_key=\"sub_source\",\n",
        "    model_name=\"baseline1_test\"\n",
        ")\n",
        "display(baseline1_sub_test_df)"
      ],
      "metadata": {
        "id": "mKcIg8J36-Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"baseline1_by_sub_source_test.json\", \"w\") as f:\n",
        "    json.dump(baseline1_sub_test_df.to_dict(\"records\"), f, indent=2)"
      ],
      "metadata": {
        "id": "NspjRKOc7GPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline 2: Pretrained model（without perplexity）"
      ],
      "metadata": {
        "id": "orN8QPlbxk7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_deberta_baseline(model, data_list, tokenizer, batch_size=32, threshold=0.4):\n",
        "\n",
        "    model.eval()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(data_list), batch_size), desc=\"DeBERTa Baseline\"):\n",
        "            batch = data_list[i:i+batch_size]\n",
        "            texts = [item['text'] for item in batch]\n",
        "            labels = [item['label'] for item in batch]\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = tokenizer(\n",
        "                texts,\n",
        "                max_length=512,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                return_tensors='pt'\n",
        "            ).to(device)\n",
        "\n",
        "            # Forward\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            probs = torch.softmax(logits, dim=1)[:, 0].cpu().numpy()  # P(AI)\n",
        "\n",
        "            # Threshold-based prediction\n",
        "            preds = (probs > threshold).astype(int)\n",
        "\n",
        "            all_probs.extend(probs)\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    results = {\n",
        "        'accuracy': accuracy_score(all_labels, all_preds),\n",
        "        'precision': precision_score(all_labels, all_preds, zero_division=0),\n",
        "        'recall': recall_score(all_labels, all_preds, zero_division=0),\n",
        "        'f1': f1_score(all_labels, all_preds, zero_division=0),\n",
        "        'auc': roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0.0\n",
        "    }\n",
        "\n",
        "    return results, all_preds, all_probs\n"
      ],
      "metadata": {
        "id": "x7cWOqKeR-Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Baseline 2: Pretrained DeBERTa (No Perplexity) ===\")\n",
        "print(\"Evaluating on validation set...\")\n",
        "deberta_results, deberta_preds, deberta_probs = evaluate_deberta_baseline(\n",
        "    pretrained_model,\n",
        "    val_data,\n",
        "    tokenizer,\n",
        "    threshold=0.3  #selected based on the best f1 of val set\n",
        ")\n",
        "\n",
        "print(f\"\\n Validation Results (threshold={0.3:.2f}):\")\n",
        "print(f\"  Accuracy:  {deberta_results['accuracy']:.4f}\")\n",
        "print(f\"  Precision: {deberta_results['precision']:.4f}\")\n",
        "print(f\"  Recall:    {deberta_results['recall']:.4f}\")\n",
        "print(f\"  F1-Score:  {deberta_results['f1']:.4f}\")\n",
        "print(f\"  AUC:       {deberta_results['auc']:.4f}\")"
      ],
      "metadata": {
        "id": "Ja5b-Hl3yL-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating on test set...\")\n",
        "deberta_test_results, deberta_test_preds, deberta_test_probs = evaluate_deberta_baseline(\n",
        "    pretrained_model,\n",
        "    test_data,\n",
        "    tokenizer,\n",
        "    threshold=0.3 #selected based on the best f1 perf on validation set\n",
        ")\n",
        "\n",
        "print(f\"\\nTest Results (threshold={0.3:.2f}):\") # Corrected threshold display\n",
        "print(f\"  Accuracy:  {deberta_test_results['accuracy']:.4f}\")\n",
        "print(f\"  Precision: {deberta_test_results['precision']:.4f}\")\n",
        "print(f\"  Recall:    {deberta_test_results['recall']:.4f}\")\n",
        "print(f\"  F1-Score:  {deberta_test_results['f1']:.4f}\")\n",
        "print(f\"  AUC:       {deberta_test_results['auc']:.4f}\")"
      ],
      "metadata": {
        "id": "ro6AEsfJyU7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate by sub-source (on validation set)"
      ],
      "metadata": {
        "id": "l0D1E2Dlz56h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deberta_labels = [d[\"label\"] for d in val_data]\n",
        "\n",
        "deberta_sub_df = evaluate_by_group(\n",
        "    data_list=val_data,\n",
        "    labels=deberta_labels,\n",
        "    preds=deberta_preds,\n",
        "    probs=deberta_probs,\n",
        "    group_key=\"sub_source\",\n",
        "    model_name=\"baseline2\"\n",
        ")\n",
        "\n",
        "display(deberta_sub_df)"
      ],
      "metadata": {
        "id": "9GTVfEIx0APx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"baseline2_by_sub_source.json\", \"w\") as f:\n",
        "    json.dump(deberta_sub_df.to_dict(\"records\"), f, indent=2)"
      ],
      "metadata": {
        "id": "w00h7a4H0SvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate by sub-source (on test set)"
      ],
      "metadata": {
        "id": "tOXxySEa7LfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deberta_labels = [d[\"label\"] for d in test_data]\n",
        "deberta_sub_df_test = evaluate_by_group(\n",
        "    data_list=test_data,\n",
        "    labels=deberta_labels,\n",
        "    preds=deberta_test_preds,\n",
        "    probs=deberta_test_probs,\n",
        "    group_key=\"sub_source\",\n",
        "    model_name=\"baseline2_test\"\n",
        ")\n",
        "\n",
        "display(deberta_sub_df_test)"
      ],
      "metadata": {
        "id": "boKEXzgq7QHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"baseline2_by_sub_source_test.json\", \"w\") as f:\n",
        "    json.dump(deberta_sub_df_test.to_dict(\"records\"), f, indent=2)"
      ],
      "metadata": {
        "id": "P0V6V8HM7TgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debug & Search best threshold"
      ],
      "metadata": {
        "id": "GZps0f_1yfNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check model label mapping\n",
        "print(\"=== Checking Model Configuration ===\")\n",
        "print(f\"Model config - id2label: {pretrained_model.config.id2label}\")\n",
        "print(f\"Model config - label2id: {pretrained_model.config.label2id}\")\n",
        "\n",
        "# Check a few predictions\n",
        "sample_texts = [train_data[0]['text'], train_data[1]['text'],  train_data[2]['text'], train_data[3]['text'], train_data[4]['text'], train_data[5]['text']]\n",
        "sample_labels = [train_data[0]['label'], train_data[1]['label'], train_data[2]['label'], train_data[3]['label'], train_data[4]['label'], train_data[5]['label']]\n",
        "\n",
        "inputs = tokenizer(sample_texts, max_length=512, truncation=True, padding='max_length', return_tensors='pt')\n",
        "if torch.cuda.is_available():\n",
        "    inputs = inputs.to('cuda')\n",
        "    pretrained_model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = pretrained_model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "print(\"\\nSample predictions:\")\n",
        "for i in range(len(sample_texts)):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"  True label: {sample_labels[i]} ({'Human' if sample_labels[i]==0 else 'AI'})\")\n",
        "    print(f\"  Probs [Human, AI]: [{probs[i][0]:.4f}, {probs[i][1]:.4f}]\")\n",
        "    print(f\"  Predicted: {np.argmax(probs[i])} ({'Human' if np.argmax(probs[i])==0 else 'AI'})\")"
      ],
      "metadata": {
        "id": "D1WZzGBe_9d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# debug\n",
        "print(f\"\\n=== Probability Distribution ===\")\n",
        "print(f\"  Min:  {np.min(deberta_probs):.4f}\")\n",
        "print(f\"  Max:  {np.max(deberta_probs):.4f}\")\n",
        "print(f\"  Mean: {np.mean(deberta_probs):.4f}\")\n",
        "print(f\"  Std:  {np.std(deberta_probs):.4f}\")\n",
        "print(f\"  Median: {np.median(deberta_probs):.4f}\")\n",
        "print(f\"\\nPercentiles:\")\n",
        "for p in [10, 25, 50, 75, 90, 95, 99]:\n",
        "    print(f\"  {p}th: {np.percentile(deberta_probs, p):.4f}\")\n",
        "\n",
        "# debug\n",
        "print(f\"\\n=== Prediction Analysis (threshold={0.3:.2f}) ===\") # Corrected threshold display\n",
        "print(f\"  Predicted as Human (0): {np.sum(np.array(deberta_preds) == 0)} ({np.sum(np.array(deberta_preds) == 0)/len(deberta_preds)*100:.1f}%)\") # Cast to np array\n",
        "print(f\"  Predicted as AI (1):    {np.sum(np.array(deberta_preds) == 1)} ({np.sum(np.array(deberta_preds) == 1)/len(deberta_preds)*100:.1f}%)\") # Cast to np array\n",
        "print(f\"\\n  Actual Human (0): {np.sum(np.array([d['label'] for d in test_data]) == 0)}\")\n",
        "print(f\"  Actual AI (1):    {np.sum(np.array([d['label'] for d in test_data]) == 1)}\")\n",
        "\n",
        "print(f\"\\nTest Results (threshold={0.3:.2f}):\") # Corrected threshold display\n",
        "print(f\"  Accuracy:  {deberta_results['accuracy']:.4f}\")\n",
        "print(f\"  Precision: {deberta_results['precision']:.4f}\")\n",
        "print(f\"  Recall:    {deberta_results['recall']:.4f}\")\n",
        "print(f\"  F1-Score:  {deberta_results['f1']:.4f}\")\n",
        "print(f\"  AUC:       {deberta_results['auc']:.4f}\")"
      ],
      "metadata": {
        "id": "ScxmQGylyZ8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# debug\n",
        "# threshold analysis\n",
        "print(\"=== DeBERTa Baseline: Threshold Sensitivity Analysis ===\")\n",
        "\n",
        "print(\"\\n1. Testing Different Thresholds:\")\n",
        "print(f\"{'Threshold':<12} {'Accuracy':<10} {'Precision':<11} {'Recall':<9} {'F1-Score':<10} {'AUC':<10} {'Strategy':<20}\")\n",
        "print(\"-\" * 95)\n",
        "\n",
        "test_thresholds_deberta = [\n",
        "    (0.3, \"Low (0.3)\"),\n",
        "    (0.4, \"Medium-Low (0.4)\"),\n",
        "    (0.5, \"Default (0.5)\"),\n",
        "    (0.6, \"Medium (0.6)\"),\n",
        "    (0.7, \"Medium-High (0.7)\"),\n",
        "    (0.8, \"High (0.8)\"),\n",
        "    (0.92, \"Paper default (0.92)\"),\n",
        "    (np.median(deberta_probs), f\"Median prob\"),\n",
        "    (np.mean(deberta_probs), f\"Mean prob\"),\n",
        "]\n",
        "\n",
        "best_acc_deberta = 0\n",
        "best_acc_threshold_deberta = None\n",
        "best_f1_deberta = 0\n",
        "best_f1_threshold_deberta = None\n",
        "\n",
        "threshold_results = []\n",
        "\n",
        "for threshold, strategy in test_thresholds_deberta:\n",
        "    preds = (np.array(deberta_probs) > threshold).astype(int)\n",
        "    val_labels_array = np.array([d['label'] for d in val_data])\n",
        "\n",
        "    acc = accuracy_score(val_labels_array, preds)\n",
        "    prec = precision_score(val_labels_array, preds, zero_division=0)\n",
        "    rec = recall_score(val_labels_array, preds, zero_division=0)\n",
        "    f1 = f1_score(val_labels_array, preds, zero_division=0)\n",
        "    auc = deberta_results['auc']\n",
        "\n",
        "    threshold_results.append({\n",
        "        'threshold': threshold,\n",
        "        'accuracy': acc,\n",
        "        'precision': prec,\n",
        "        'recall': rec,\n",
        "        'f1': f1,\n",
        "        'auc': auc\n",
        "    })\n",
        "\n",
        "    print(f\"{threshold:<12.4f} {acc:<10.4f} {prec:<11.4f} {rec:<9.4f} {f1:<10.4f} {auc:<10.4f} {strategy:<20}\")\n",
        "\n",
        "    if acc > best_acc_deberta:\n",
        "        best_acc_deberta = acc\n",
        "        best_acc_threshold_deberta = threshold\n",
        "    if f1 > best_f1_deberta:\n",
        "        best_f1_deberta = f1\n",
        "        best_f1_threshold_deberta = threshold\n",
        "\n",
        "print(\"\\n2. Best Results:\")\n",
        "print(f\"  Best Accuracy:  {best_acc_deberta:.4f} at threshold={best_acc_threshold_deberta:.4f}\")\n",
        "print(f\"  Best F1-Score:  {best_f1_deberta:.4f} at threshold={best_f1_threshold_deberta:.4f}\")\n",
        "print(f\"  Current (0.92): Acc={deberta_results['accuracy']:.4f}, F1={deberta_results['f1']:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n3. Generating Threshold Analysis Plots...\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# try finer range\n",
        "threshold_range = np.linspace(0.1, 0.99, 50)\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1s = []\n",
        "\n",
        "for t in threshold_range:\n",
        "    preds = (np.array(deberta_probs) > t).astype(int)\n",
        "    accuracies.append(accuracy_score(val_labels_array, preds))\n",
        "    precisions.append(precision_score(val_labels_array, preds, zero_division=0))\n",
        "    recalls.append(recall_score(val_labels_array, preds, zero_division=0))\n",
        "    f1s.append(f1_score(val_labels_array, preds, zero_division=0))\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Precision vs Recall\n",
        "axes[0, 0].plot(recalls, precisions, 'b-', linewidth=2)\n",
        "axes[0, 0].scatter([deberta_results['recall']], [deberta_results['precision']],\n",
        "                   c='red', s=100, zorder=5, label=f'Current (t=0.92)')\n",
        "if best_f1_threshold_deberta != 0.92:\n",
        "    best_result = [r for r in threshold_results if r['threshold'] == best_f1_threshold_deberta][0]\n",
        "    axes[0, 0].scatter([best_result['recall']], [best_result['precision']],\n",
        "                       c='green', s=100, marker='*', zorder=5, label=f'Best F1 (t={best_f1_threshold_deberta:.2f})')\n",
        "axes[0, 0].set_xlabel('Recall')\n",
        "axes[0, 0].set_ylabel('Precision')\n",
        "axes[0, 0].set_title('Precision-Recall Curve')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# 2. Threshold vs Accuracy/F1\n",
        "axes[0, 1].plot(threshold_range, accuracies, label='Accuracy', linewidth=2)\n",
        "axes[0, 1].plot(threshold_range, f1s, label='F1-Score', linewidth=2)\n",
        "axes[0, 1].axvline(0.92, color='red', linestyle='--', alpha=0.7, label='Current (0.92)')\n",
        "if best_f1_threshold_deberta != 0.92:\n",
        "    axes[0, 1].axvline(best_f1_threshold_deberta, color='green', linestyle='--', alpha=0.7, label=f'Best F1 ({best_f1_threshold_deberta:.2f})')\n",
        "axes[0, 1].set_xlabel('Threshold')\n",
        "axes[0, 1].set_ylabel('Score')\n",
        "axes[0, 1].set_title('Threshold vs Accuracy/F1')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Threshold vs Precision/Recall\n",
        "axes[1, 0].plot(threshold_range, precisions, label='Precision', linewidth=2, color='blue')\n",
        "axes[1, 0].plot(threshold_range, recalls, label='Recall', linewidth=2, color='orange')\n",
        "axes[1, 0].axvline(0.92, color='red', linestyle='--', alpha=0.7, label='Current (0.92)')\n",
        "if best_f1_threshold_deberta != 0.92:\n",
        "    axes[1, 0].axvline(best_f1_threshold_deberta, color='green', linestyle='--', alpha=0.7, label=f'Best F1 ({best_f1_threshold_deberta:.2f})')\n",
        "axes[1, 0].set_xlabel('Threshold')\n",
        "axes[1, 0].set_ylabel('Score')\n",
        "axes[1, 0].set_title('Threshold vs Precision/Recall')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. prob distribution by labels\n",
        "human_probs_deberta = [deberta_probs[i] for i in range(len(deberta_probs)) if val_data[i]['label'] == 0]\n",
        "ai_probs_deberta = [deberta_probs[i] for i in range(len(deberta_probs)) if val_data[i]['label'] == 1]\n",
        "\n",
        "axes[1, 1].hist(human_probs_deberta, bins=30, alpha=0.5, label='Human', color='blue', density=True)\n",
        "axes[1, 1].hist(ai_probs_deberta, bins=30, alpha=0.5, label='AI', color='orange', density=True)\n",
        "axes[1, 1].axvline(0.92, color='red', linestyle='--', linewidth=2, label='Current (0.92)')\n",
        "if best_f1_threshold_deberta != 0.92:\n",
        "    axes[1, 1].axvline(best_f1_threshold_deberta, color='green', linestyle='--', linewidth=2, label=f'Best ({best_f1_threshold_deberta:.2f})')\n",
        "axes[1, 1].set_xlabel('Predicted Probability (AI)')\n",
        "axes[1, 1].set_ylabel('Density')\n",
        "axes[1, 1].set_title('Probability Distribution by Label')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('deberta_threshold_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Threshold analysis complete. Plot saved to 'deberta_threshold_analysis.png'\")\n",
        "\n",
        "print(\"\\n4. Key Insights:\")\n",
        "print(f\"  • Human prob - Mean: {np.mean(human_probs_deberta):.4f}, Std: {np.std(human_probs_deberta):.4f}\")\n",
        "print(f\"  • AI prob    - Mean: {np.mean(ai_probs_deberta):.4f}, Std: {np.std(ai_probs_deberta):.4f}\")\n",
        "print(f\"  • Gap: {np.mean(ai_probs_deberta) - np.mean(human_probs_deberta):.4f}\")\n",
        "\n",
        "# distribution overlap\n",
        "overlap_deberta = len([p for p in human_probs_deberta if p > np.median(ai_probs_deberta)])\n",
        "print(f\"  • Overlap: {overlap_deberta/len(human_probs_deberta)*100:.1f}% of Human samples > AI median\")\n",
        "\n",
        "if best_f1_threshold_deberta == 0.5:\n",
        "    print(f\"  • Threshold 0.5 gives best balance (standard logits interpretation)\")\n",
        "elif best_f1_threshold_deberta < 0.5:\n",
        "    print(f\"  • Best threshold < 0.5 suggests model is biased toward predicting Human\")\n",
        "else:\n",
        "    print(f\"  • Best threshold > 0.5 suggests model is biased toward predicting AI\")\n",
        "\n",
        "if deberta_results['f1'] == 0:\n",
        "    print(f\"  • Current threshold  is TOO HIGH - causing zero F1!\")\n",
        "    print(f\"  • Recommendation: Use threshold={best_f1_threshold_deberta:.2f} for {best_f1_deberta:.1%} F1\")"
      ],
      "metadata": {
        "id": "S-dpCKS7yrWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# debug\n",
        "# threshold analysis\n",
        "print(\"=== DeBERTa Baseline: Threshold Sensitivity Analysis ===\")\n",
        "\n",
        "print(\"\\n1. Testing Different Thresholds:\")\n",
        "print(f\"{'Threshold':<12} {'Accuracy':<10} {'Precision':<11} {'Recall':<9} {'F1-Score':<10} {'AUC':<10} {'Strategy':<20}\")\n",
        "print(\"-\" * 95)\n",
        "\n",
        "test_thresholds_deberta = [\n",
        "    (0.3, \"Low (0.3)\"),\n",
        "    (0.4, \"Medium-Low (0.4)\"),\n",
        "    (0.5, \"Default (0.5)\"),\n",
        "    (0.6, \"Medium (0.6)\"),\n",
        "    (0.7, \"Medium-High (0.7)\"),\n",
        "    (0.8, \"High (0.8)\"),\n",
        "    (0.92, \"Paper default (0.92)\"),\n",
        "    (np.median(deberta_probs), f\"Median prob\"),\n",
        "    (np.mean(deberta_probs), f\"Mean prob\"),\n",
        "]\n",
        "\n",
        "best_acc_deberta = 0\n",
        "best_acc_threshold_deberta = None\n",
        "best_f1_deberta = 0\n",
        "best_f1_threshold_deberta = None\n",
        "\n",
        "threshold_results = []\n",
        "\n",
        "for threshold, strategy in test_thresholds_deberta:\n",
        "    preds = (np.array(deberta_probs) > threshold).astype(int)\n",
        "    val_labels_array = np.array([d['label'] for d in test_data])\n",
        "\n",
        "    acc = accuracy_score(val_labels_array, preds)\n",
        "    prec = precision_score(val_labels_array, preds, zero_division=0)\n",
        "    rec = recall_score(val_labels_array, preds, zero_division=0)\n",
        "    f1 = f1_score(val_labels_array, preds, zero_division=0)\n",
        "    auc = deberta_results['auc']\n",
        "\n",
        "    threshold_results.append({\n",
        "        'threshold': threshold,\n",
        "        'accuracy': acc,\n",
        "        'precision': prec,\n",
        "        'recall': rec,\n",
        "        'f1': f1,\n",
        "        'auc': auc\n",
        "    })\n",
        "\n",
        "    print(f\"{threshold:<12.4f} {acc:<10.4f} {prec:<11.4f} {rec:<9.4f} {f1:<10.4f} {auc:<10.4f} {strategy:<20}\")\n",
        "\n",
        "    if acc > best_acc_deberta:\n",
        "        best_acc_deberta = acc\n",
        "        best_acc_threshold_deberta = threshold\n",
        "    if f1 > best_f1_deberta:\n",
        "        best_f1_deberta = f1\n",
        "        best_f1_threshold_deberta = threshold\n",
        "\n",
        "print(\"\\n2. Best Results:\")\n",
        "print(f\"  Best Accuracy:  {best_acc_deberta:.4f} at threshold={best_acc_threshold_deberta:.4f}\")\n",
        "print(f\"  Best F1-Score:  {best_f1_deberta:.4f} at threshold={best_f1_threshold_deberta:.4f}\")\n",
        "print(f\"  Current (0.92): Acc={deberta_results['accuracy']:.4f}, F1={deberta_results['f1']:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n3. Generating Threshold Analysis Plots...\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# try finer range\n",
        "threshold_range = np.linspace(0.1, 0.99, 50)\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1s = []\n",
        "\n",
        "for t in threshold_range:\n",
        "    preds = (np.array(deberta_probs) > t).astype(int)\n",
        "    accuracies.append(accuracy_score(val_labels_array, preds))\n",
        "    precisions.append(precision_score(val_labels_array, preds, zero_division=0))\n",
        "    recalls.append(recall_score(val_labels_array, preds, zero_division=0))\n",
        "    f1s.append(f1_score(val_labels_array, preds, zero_division=0))\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Precision vs Recall\n",
        "axes[0, 0].plot(recalls, precisions, 'b-', linewidth=2)\n",
        "axes[0, 0].scatter([deberta_results['recall']], [deberta_results['precision']],\n",
        "                   c='red', s=100, zorder=5, label=f'Current (t=0.92)')\n",
        "if best_f1_threshold_deberta != 0.92:\n",
        "    best_result = [r for r in threshold_results if r['threshold'] == best_f1_threshold_deberta][0]\n",
        "    axes[0, 0].scatter([best_result['recall']], [best_result['precision']],\n",
        "                       c='green', s=100, marker='*', zorder=5, label=f'Best F1 (t={best_f1_threshold_deberta:.2f})')\n",
        "axes[0, 0].set_xlabel('Recall')\n",
        "axes[0, 0].set_ylabel('Precision')\n",
        "axes[0, 0].set_title('Precision-Recall Curve')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# 2. Threshold vs Accuracy/F1\n",
        "axes[0, 1].plot(threshold_range, accuracies, label='Accuracy', linewidth=2)\n",
        "axes[0, 1].plot(threshold_range, f1s, label='F1-Score', linewidth=2)\n",
        "axes[0, 1].axvline(0.92, color='red', linestyle='--', alpha=0.7, label='Current (0.92)')\n",
        "if best_f1_threshold_deberta != 0.92:\n",
        "    axes[0, 1].axvline(best_f1_threshold_deberta, color='green', linestyle='--', alpha=0.7, label=f'Best F1 ({best_f1_threshold_deberta:.2f})')\n",
        "axes[0, 1].set_xlabel('Threshold')\n",
        "axes[0, 1].set_ylabel('Score')\n",
        "axes[0, 1].set_title('Threshold vs Accuracy/F1')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Threshold vs Precision/Recall\n",
        "axes[1, 0].plot(threshold_range, precisions, label='Precision', linewidth=2, color='blue')\n",
        "axes[1, 0].plot(threshold_range, recalls, label='Recall', linewidth=2, color='orange')\n",
        "axes[1, 0].axvline(0.92, color='red', linestyle='--', alpha=0.7, label='Current (0.92)')\n",
        "if best_f1_threshold_deberta != 0.92:\n",
        "    axes[1, 0].axvline(best_f1_threshold_deberta, color='green', linestyle='--', alpha=0.7, label=f'Best F1 ({best_f1_threshold_deberta:.2f})')\n",
        "axes[1, 0].set_xlabel('Threshold')\n",
        "axes[1, 0].set_ylabel('Score')\n",
        "axes[1, 0].set_title('Threshold vs Precision/Recall')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. prob distribution by labels\n",
        "human_probs_deberta = [deberta_probs[i] for i in range(len(deberta_probs)) if val_data[i]['label'] == 0]\n",
        "ai_probs_deberta = [deberta_probs[i] for i in range(len(deberta_probs)) if val_data[i]['label'] == 1]\n",
        "\n",
        "axes[1, 1].hist(human_probs_deberta, bins=30, alpha=0.5, label='Human', color='blue', density=True)\n",
        "axes[1, 1].hist(ai_probs_deberta, bins=30, alpha=0.5, label='AI', color='orange', density=True)\n",
        "axes[1, 1].axvline(0.92, color='red', linestyle='--', linewidth=2, label='Current (0.92)')\n",
        "if best_f1_threshold_deberta != 0.92:\n",
        "    axes[1, 1].axvline(best_f1_threshold_deberta, color='green', linestyle='--', linewidth=2, label=f'Best ({best_f1_threshold_deberta:.2f})')\n",
        "axes[1, 1].set_xlabel('Predicted Probability (AI)')\n",
        "axes[1, 1].set_ylabel('Density')\n",
        "axes[1, 1].set_title('Probability Distribution by Label')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('deberta_threshold_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Threshold analysis complete. Plot saved to 'deberta_threshold_analysis.png'\")\n",
        "\n",
        "print(\"\\n4. Key Insights:\")\n",
        "print(f\"  • Human prob - Mean: {np.mean(human_probs_deberta):.4f}, Std: {np.std(human_probs_deberta):.4f}\")\n",
        "print(f\"  • AI prob    - Mean: {np.mean(ai_probs_deberta):.4f}, Std: {np.std(ai_probs_deberta):.4f}\")\n",
        "print(f\"  • Gap: {np.mean(ai_probs_deberta) - np.mean(human_probs_deberta):.4f}\")\n",
        "\n",
        "# distribution overlap\n",
        "overlap_deberta = len([p for p in human_probs_deberta if p > np.median(ai_probs_deberta)])\n",
        "print(f\"  • Overlap: {overlap_deberta/len(human_probs_deberta)*100:.1f}% of Human samples > AI median\")\n",
        "\n",
        "if best_f1_threshold_deberta == 0.5:\n",
        "    print(f\"  • Threshold 0.5 gives best balance (standard logits interpretation)\")\n",
        "elif best_f1_threshold_deberta < 0.5:\n",
        "    print(f\"  • Best threshold < 0.5 suggests model is biased toward predicting Human\")\n",
        "else:\n",
        "    print(f\"  • Best threshold > 0.5 suggests model is biased toward predicting AI\")\n",
        "\n",
        "if deberta_results['f1'] == 0:\n",
        "    print(f\"  • Current threshold  is TOO HIGH - causing zero F1!\")\n",
        "    print(f\"  • Recommendation: Use threshold={best_f1_threshold_deberta:.2f} for {best_f1_deberta:.1%} F1\")"
      ],
      "metadata": {
        "id": "HUD2SAzCGz49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Train\n"
      ],
      "metadata": {
        "id": "6nBcmHDivMDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare data\n",
        "train_dataset = MGTDatasetWithPerplexity(train_data, train_scores_normalized, tokenizer)\n",
        "val_dataset = MGTDatasetWithPerplexity(val_data, val_scores_normalized, tokenizer)\n",
        "test_dataset = MGTDatasetWithPerplexity(test_data, test_scores_normalized, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"DataLoaders created\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")"
      ],
      "metadata": {
        "id": "-m_xK6RqvPeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW # Use torch.optim.AdamW\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "new_model.to(device)\n",
        "\n",
        "print(\"=== Training Classifier Head Only ===\")\n",
        "\n",
        "for param in new_model.deberta.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in new_model.pooler.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, new_model.parameters()), lr=3e-4)\n",
        "num_epochs_stage1 = 1\n",
        "total_steps = len(train_loader) * num_epochs_stage1\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=50, num_training_steps=total_steps)\n",
        "\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in new_model.parameters() if p.requires_grad):,}\")\n",
        "print(f\"Training for {num_epochs_stage1} epoch(s), {total_steps} steps\")"
      ],
      "metadata": {
        "id": "8rWdqYuFvSXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "new_model.train()\n",
        "best_val_f1 = 0\n",
        "\n",
        "for epoch in range(num_epochs_stage1):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs_stage1}\")\n",
        "    epoch_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Training\")\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        perplexity_feature = batch['perplexity_feature'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = new_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            perplexity_feature=perplexity_feature,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # validation\n",
        "    new_model.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            perplexity_feature = batch['perplexity_feature'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = new_model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                perplexity_feature=perplexity_feature\n",
        "            )\n",
        "\n",
        "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "            val_preds.extend(preds)\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_f1 = f1_score(val_labels, val_preds)\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    print(f\"Validation - Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save(new_model.state_dict(), 'best_model_stage1.pt')\n",
        "        print(f\"Best model saved (F1={val_f1:.4f})\")\n",
        "\n",
        "    new_model.train()\n",
        "\n",
        "print(f\"\\n Train Complete. Best Val F1: {best_val_f1:.4f}\")"
      ],
      "metadata": {
        "id": "S_4-f7HpvZSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Further evaluation"
      ],
      "metadata": {
        "id": "m8zRY3v5vdPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "pxG6S7x3cMJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download saved model trained before\n",
        "!gdown --id 1mJpvunZyy32sw7MLqLothu7cE6nvCD60 -O best_model_stage1.pt"
      ],
      "metadata": {
        "id": "7d9fwRjdcQrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf best_model_stage1.pt"
      ],
      "metadata": {
        "id": "makIVOS0chs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model for further evaluation\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "new_model.load_state_dict(torch.load('/content/best_model_stage1.pt'))\n",
        "new_model.eval()\n",
        "\n",
        "def evaluate_model_with_perplexity(model, data_list, perplexity_scores, tokenizer, batch_size=32):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    all_sources = []\n",
        "    all_sub_sources = []\n",
        "    all_model = []\n",
        "\n",
        "    dataset = MGTDatasetWithPerplexity(data_list, perplexity_scores, tokenizer)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            perplexity_feature = batch['perplexity_feature'].to(device)\n",
        "            labels = batch['labels']\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                perplexity_feature=perplexity_feature\n",
        "            )\n",
        "\n",
        "            logits = outputs.logits\n",
        "            probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.numpy())\n",
        "            all_probs.extend(probs)\n",
        "            all_sources.extend(batch['source'])\n",
        "            all_sub_sources.extend(batch['sub_source'])\n",
        "            all_model.extend(batch['model'])\n",
        "\n",
        "\n",
        "    # Overall metrics\n",
        "    results = {\n",
        "        'accuracy': accuracy_score(all_labels, all_preds),\n",
        "        'precision': precision_score(all_labels, all_preds, zero_division=0),\n",
        "        'recall': recall_score(all_labels, all_preds, zero_division=0),\n",
        "        'f1': f1_score(all_labels, all_preds, zero_division=0),\n",
        "        'auc': roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0.0\n",
        "    }\n",
        "\n",
        "    return results, all_preds, all_probs, all_sources, all_sub_sources, all_model"
      ],
      "metadata": {
        "id": "DJuwchK7vh8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on validation set"
      ],
      "metadata": {
        "id": "SykelXvz1ePo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### key metrics"
      ],
      "metadata": {
        "id": "EtLzoxfuQwTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_results, final_preds, final_probs, final_sources, final_sub_sources, final_models = evaluate_model_with_perplexity(\n",
        "    new_model,\n",
        "    val_data,\n",
        "    val_scores_normalized,\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "print(\"\\n=== Overall Results ===\")\n",
        "print(f\"Accuracy:  {final_results['accuracy']:.4f}\")\n",
        "print(f\"Precision: {final_results['precision']:.4f}\")\n",
        "print(f\"Recall:    {final_results['recall']:.4f}\")\n",
        "print(f\"F1-Score:  {final_results['f1']:.4f}\")\n",
        "print(f\"AUC:       {final_results['auc']:.4f}\")"
      ],
      "metadata": {
        "id": "9EB2dYsj11QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### by sub-source"
      ],
      "metadata": {
        "id": "nr9Dv2GZ1kVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_labels = [d[\"label\"] for d in val_data]\n",
        "\n",
        "hybrid_sub_df = evaluate_by_group(\n",
        "    data_list=val_data,\n",
        "    labels=final_labels,\n",
        "    preds=final_preds,\n",
        "    probs=final_probs,\n",
        "    group_key=\"sub_source\",\n",
        "    model_name=\"Intergrated model\"\n",
        ")\n",
        "display(hybrid_sub_df)"
      ],
      "metadata": {
        "id": "qiv5d1ap1m_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"hybridmodel_by_domain.json\", \"w\") as f:\n",
        "    json.dump(hybrid_sub_df.to_dict(\"records\"), f, indent=2)"
      ],
      "metadata": {
        "id": "DdKZ27361pk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### by source"
      ],
      "metadata": {
        "id": "bngzATcj2FsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_labels = [d[\"label\"] for d in val_data]\n",
        "hybrid_souce_df = evaluate_by_group(\n",
        "    data_list=val_data,\n",
        "    labels=final_labels,\n",
        "    preds=final_preds,\n",
        "    probs=final_probs,\n",
        "    group_key=\"source\",\n",
        "    model_name=\"Intergrated model\"\n",
        ")\n",
        "display(hybrid_souce_df)"
      ],
      "metadata": {
        "id": "3Pz4DwZr2Hs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### by model"
      ],
      "metadata": {
        "id": "Y4Sq4FAG2Jxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_model_df = evaluate_by_group(\n",
        "    data_list=val_data,\n",
        "    labels=final_labels,\n",
        "    preds=final_preds,\n",
        "    probs=final_probs,\n",
        "    group_key=\"model\",\n",
        "    model_name=\"Intergrated model\"\n",
        ")\n",
        "display(hybrid_model_df)"
      ],
      "metadata": {
        "id": "8R9ZCAcH2L5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on test set"
      ],
      "metadata": {
        "id": "EU6SsFnPQZfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### key metrics"
      ],
      "metadata": {
        "id": "s0GXY1TZQ9Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_results_test, final_preds_test, final_probs_test, final_sources_test, final_sub_sources_test, final_models_test = evaluate_model_with_perplexity(\n",
        "    new_model,\n",
        "    test_data,\n",
        "    test_scores_normalized,\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "print(\"\\n=== Overall Results ===\")\n",
        "print(f\"Accuracy:  {final_results_test['accuracy']:.4f}\")\n",
        "print(f\"Precision: {final_results_test['precision']:.4f}\")\n",
        "print(f\"Recall:    {final_results_test['recall']:.4f}\")\n",
        "print(f\"F1-Score:  {final_results_test['f1']:.4f}\")\n",
        "print(f\"AUC:       {final_results_test['auc']:.4f}\")"
      ],
      "metadata": {
        "id": "aRxWUMab12Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### by sub-source"
      ],
      "metadata": {
        "id": "h0oEalOtQeh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_test_labels = [d[\"label\"] for d in test_data]\n",
        "hybrid_sub_test_df = evaluate_by_group(\n",
        "    data_list=test_data,\n",
        "    labels=final_test_labels,\n",
        "    preds=final_preds_test,\n",
        "    probs=final_probs_test,\n",
        "    group_key=\"sub_source\",\n",
        "    model_name=\"Intergrated model\"\n",
        ")\n",
        "display(hybrid_sub_test_df)"
      ],
      "metadata": {
        "id": "ktjwVheBQhdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"hybridmodel_by_domain_test.json\", \"w\") as f:\n",
        "    json.dump(hybrid_sub_test_df.to_dict(\"records\"), f, indent=2)"
      ],
      "metadata": {
        "id": "BCSoNE_QRJ5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### by source"
      ],
      "metadata": {
        "id": "RpwXtTFUQndk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_souce_test_df = evaluate_by_group(\n",
        "    data_list=test_data,\n",
        "    labels=final_test_labels,\n",
        "    preds=final_preds_test,\n",
        "    probs=final_probs_test,\n",
        "    group_key=\"source\",\n",
        "    model_name=\"Intergrated model\"\n",
        ")\n",
        "display(hybrid_souce_test_df)"
      ],
      "metadata": {
        "id": "HJKAJhPCRFKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### by model"
      ],
      "metadata": {
        "id": "_Am3s8B7RUZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_model_test_df = evaluate_by_group(\n",
        "    data_list=test_data,\n",
        "    labels=final_test_labels,\n",
        "    preds=final_preds_test,\n",
        "    probs=final_probs_test,\n",
        "    group_key=\"model\",\n",
        "    model_name=\"Intergrated model\"\n",
        ")\n",
        "display(hybrid_model_test_df)"
      ],
      "metadata": {
        "id": "53EScEvtRVjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overall Comparison"
      ],
      "metadata": {
        "id": "-q4SOyWX2PQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### validation set"
      ],
      "metadata": {
        "id": "9R0w6jWh2Vj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # remove FutureWarning\n",
        "\n",
        "# ---------- 1) load ----------\n",
        "paths = {\n",
        "    \"baseline1\": \"/content/baseline1_by_sub_source.json\",\n",
        "    \"baseline2\": \"/content/baseline2_by_sub_source.json\",\n",
        "    \"hybrid\":    \"/content/hybridmodel_by_domain.json\",\n",
        "}\n",
        "\n",
        "dfs = []\n",
        "for name, p in paths.items():\n",
        "    with open(p, \"r\") as f:\n",
        "        rows = json.load(f)\n",
        "    df = pd.DataFrame(rows)\n",
        "    df[\"model_name\"] = name\n",
        "    dfs.append(df)\n",
        "\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "group_col = \"sub_source\"\n",
        "\n",
        "# ---------- 2) choose x-axis order ----------\n",
        "order = (df_all[df_all[\"model_name\"]==\"baseline2\"]\n",
        "         .sort_values(\"count\", ascending=False)[group_col]\n",
        "         .tolist())\n",
        "\n",
        "all_groups = df_all[group_col].unique().tolist()\n",
        "for g in all_groups:\n",
        "    if g not in order:\n",
        "        order.append(g)\n",
        "\n",
        "df_all[group_col] = pd.Categorical(df_all[group_col], categories=order, ordered=True)\n",
        "\n",
        "# ---------- 3) pivot to wide table for plotting ----------\n",
        "metrics = [\"f1\", \"accuracy\", \"precision\", \"recall\"]\n",
        "wide = {}\n",
        "for m in metrics:\n",
        "    wide[m] = df_all.pivot_table(index=group_col, columns=\"model_name\", values=m, aggfunc=\"first\", observed=True).sort_index()\n",
        "\n",
        "# Extract count (data size) for each subsource\n",
        "count_per_group = df_all[df_all[\"model_name\"]==\"baseline2\"].set_index(group_col)[\"count\"]\n",
        "\n",
        "# ---------- 4) plot with dual axes ----------\n",
        "def plot_metric_with_count(metric_name, ylabel=None, save_path=None):\n",
        "    w = wide[metric_name].copy()\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(14, 5))\n",
        "\n",
        "    # Left y-axis: metrics\n",
        "    ax1.set_xlabel(group_col)\n",
        "    ax1.set_ylabel(ylabel or metric_name, color=\"black\")\n",
        "    ax1.set_ylim(0, 1.0)\n",
        "\n",
        "    for model_name in [\"baseline1\", \"baseline2\", \"hybrid\"]:\n",
        "        if model_name in w.columns:\n",
        "            ax1.plot(w.index.astype(str), w[model_name], marker=\"o\", label=model_name, linewidth=2)\n",
        "\n",
        "    ax1.tick_params(axis=\"y\")\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Right y-axis: count\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.set_ylabel(\"Sample Count\", color=\"gray\")\n",
        "    x_pos = np.arange(len(w.index))\n",
        "    bars = ax2.bar(x_pos, count_per_group[w.index].values, alpha=0.2, color=\"gray\", label=\"Sample Count\")\n",
        "    ax2.tick_params(axis=\"y\", labelcolor=\"gray\")\n",
        "\n",
        "    # Add count labels on bars\n",
        "    for i, (pos, count) in enumerate(zip(x_pos, count_per_group[w.index].values)):\n",
        "        ax2.text(pos, count, str(int(count)), ha=\"center\", va=\"bottom\", fontsize=8, color=\"gray\")\n",
        "\n",
        "    ax1.set_xticks(x_pos)\n",
        "    ax1.set_xticklabels(w.index.astype(str), rotation=45, ha=\"right\")\n",
        "\n",
        "    # Legend\n",
        "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper left\")\n",
        "\n",
        "    plt.title(f\"{metric_name.upper()} by {group_col} (with Sample Count)\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=200)\n",
        "    plt.show()\n",
        "\n",
        "plot_metric_with_count(\"f1\",        \"F1\",        \"/content/compare_f1.png\")\n",
        "plot_metric_with_count(\"accuracy\",  \"Accuracy\",  \"/content/compare_accuracy.png\")\n",
        "plot_metric_with_count(\"precision\", \"Precision\", \"/content/compare_precision.png\")\n",
        "plot_metric_with_count(\"recall\",    \"Recall\",    \"/content/compare_recall.png\")"
      ],
      "metadata": {
        "id": "QVcJREgtXLtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test set"
      ],
      "metadata": {
        "id": "GvtG6c5p2XbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # remove FutureWarning\n",
        "\n",
        "# ---------- 1) load ----------\n",
        "paths = {\n",
        "    \"baseline1\": \"/content/baseline1_by_sub_source_test.json\",\n",
        "    \"baseline2\": \"/content/baseline2_by_sub_source_test.json\",\n",
        "    \"hybrid\":    \"/content/hybridmodel_by_domain_test.json\",\n",
        "}\n",
        "\n",
        "dfs = []\n",
        "for name, p in paths.items():\n",
        "    with open(p, \"r\") as f:\n",
        "        rows = json.load(f)\n",
        "    df = pd.DataFrame(rows)\n",
        "    df[\"model_name\"] = name\n",
        "    dfs.append(df)\n",
        "\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "group_col = \"sub_source\"\n",
        "\n",
        "# ---------- 2) choose x-axis order ----------\n",
        "order = (df_all[df_all[\"model_name\"]==\"baseline2\"]\n",
        "         .sort_values(\"count\", ascending=False)[group_col]\n",
        "         .tolist())\n",
        "\n",
        "all_groups = df_all[group_col].unique().tolist()\n",
        "for g in all_groups:\n",
        "    if g not in order:\n",
        "        order.append(g)\n",
        "\n",
        "df_all[group_col] = pd.Categorical(df_all[group_col], categories=order, ordered=True)\n",
        "\n",
        "# ---------- 3) pivot to wide table for plotting ----------\n",
        "metrics = [\"f1\", \"accuracy\", \"precision\", \"recall\"]\n",
        "wide = {}\n",
        "for m in metrics:\n",
        "    wide[m] = df_all.pivot_table(index=group_col, columns=\"model_name\", values=m, aggfunc=\"first\", observed=True).sort_index()\n",
        "\n",
        "# Extract count (data size) for each subsource\n",
        "count_per_group = df_all[df_all[\"model_name\"]==\"baseline2\"].set_index(group_col)[\"count\"]\n",
        "\n",
        "# ---------- 4) plot with dual axes ----------\n",
        "def plot_metric_with_count(metric_name, ylabel=None, save_path=None):\n",
        "    w = wide[metric_name].copy()\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(14, 5))\n",
        "\n",
        "    # Left y-axis: metrics\n",
        "    ax1.set_xlabel(group_col)\n",
        "    ax1.set_ylabel(ylabel or metric_name, color=\"black\")\n",
        "    ax1.set_ylim(0, 1.0)\n",
        "\n",
        "    for model_name in [\"baseline1\", \"baseline2\", \"hybrid\"]:\n",
        "        if model_name in w.columns:\n",
        "            ax1.plot(w.index.astype(str), w[model_name], marker=\"o\", label=model_name, linewidth=2)\n",
        "\n",
        "    ax1.tick_params(axis=\"y\")\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Right y-axis: count\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.set_ylabel(\"Sample Count\", color=\"gray\")\n",
        "    x_pos = np.arange(len(w.index))\n",
        "    bars = ax2.bar(x_pos, count_per_group[w.index].values, alpha=0.2, color=\"gray\", label=\"Sample Count\")\n",
        "    ax2.tick_params(axis=\"y\", labelcolor=\"gray\")\n",
        "\n",
        "    # Add count labels on bars\n",
        "    for i, (pos, count) in enumerate(zip(x_pos, count_per_group[w.index].values)):\n",
        "        ax2.text(pos, count, str(int(count)), ha=\"center\", va=\"bottom\", fontsize=8, color=\"gray\")\n",
        "\n",
        "    ax1.set_xticks(x_pos)\n",
        "    ax1.set_xticklabels(w.index.astype(str), rotation=45, ha=\"right\")\n",
        "\n",
        "    # Legend\n",
        "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper left\")\n",
        "\n",
        "    plt.title(f\"{metric_name.upper()} by {group_col} (with Sample Count)\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=200)\n",
        "    plt.show()\n",
        "\n",
        "plot_metric_with_count(\"f1\",        \"F1\",        \"/content/compare_f1_test.png\")\n",
        "plot_metric_with_count(\"accuracy\",  \"Accuracy\",  \"/content/compare_accuracy_test.png\")\n",
        "plot_metric_with_count(\"precision\", \"Precision\", \"/content/compare_precision_test.png\")\n",
        "plot_metric_with_count(\"recall\",    \"Recall\",    \"/content/compare_recall_test.png\")"
      ],
      "metadata": {
        "id": "dR1bg6nsYIOC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}